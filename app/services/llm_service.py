import ollama
import openai
import logging
from typing import Dict, Any

from app.core.config import settings
from app.utils.prompt_templates import RAG_PROMPT_TEMPLATE

class LLMService:
    """
    Service ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi c√°c Large Language Models (LLMs).
    H·ªó tr·ª£ Ollama l√†m l·ª±a ch·ªçn ch√≠nh v√† OpenAI l√†m ph∆∞∆°ng √°n d·ª± ph√≤ng.
    """
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.ollama_client = None
        self.openai_client = None
        self._initialize_clients()

    def _initialize_clients(self):
        """Kh·ªüi t·∫°o c√°c client cho Ollama v√† OpenAI."""
        try:
            # Th·ª≠ k·∫øt n·ªëi t·ªõi Ollama
            self.ollama_client = ollama.AsyncClient()
            self.logger.info("‚úÖ Ollama client initialized successfully.")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  Could not initialize Ollama client: {e}. Ollama will not be available.")

        if settings.OPENAI_API_KEY and settings.OPENAI_API_KEY != "your_openai_key_here":
            try:
                self.openai_client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
                self.logger.info("‚úÖ OpenAI client initialized successfully.")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è  Could not initialize OpenAI client: {e}. OpenAI will not be available.")
        else:
            self.logger.info("‚ÑπÔ∏è  OPENAI_API_KEY not set. OpenAI fallback will not be available.")

    async def generate_response(self, question: str, context: str) -> Dict[str, Any]:
        """
        T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM d·ª±a tr√™n c√¢u h·ªèi v√† context.
        ∆Øu ti√™n s·ª≠ d·ª•ng Ollama, n·∫øu th·∫•t b·∫°i s·∫Ω chuy·ªÉn sang OpenAI.
        """
        prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)
        
        # ∆Øu ti√™n Ollama
        if self.ollama_client:
            try:
                self.logger.info(f"ü§ñ Trying to generate response with Ollama model: {settings.LLM_MODEL}")
                response = await self.ollama_client.chat(
                    model=settings.LLM_MODEL,
                    messages=[{'role': 'user', 'content': prompt}]
                )
                return {
                    "answer": response['message']['content'],
                    "model_used": f"ollama/{settings.LLM_MODEL}",
                    "success": True
                }
            except Exception as e:
                self.logger.error(f"‚ùå Ollama generation failed: {e}. Trying OpenAI as fallback.")

        # Fallback sang OpenAI
        if self.openai_client:
            try:
                self.logger.info("ü§ñ Falling back to OpenAI model: gpt-3.5-turbo")
                response = await self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}]
                )
                return {
                    "answer": response.choices[0].message.content,
                    "model_used": "openai/gpt-3.5-turbo",
                    "success": True
                }
            except Exception as e:
                self.logger.error(f"‚ùå OpenAI generation also failed: {e}")

        # N·∫øu c·∫£ hai ƒë·ªÅu th·∫•t b·∫°i
        return {
            "answer": "R·∫•t ti·∫øc, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi v√†o l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.",
            "model_used": "none",
            "success": False
        }